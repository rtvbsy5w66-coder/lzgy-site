# ü§ñ AI Server Setup Guide

## 1. Nginx Auth Proxy konfigur√°ci√≥

```nginx
# /etc/nginx/sites-available/ollama
server {
    listen 80;
    server_name your-ai-server.com;

    # API Token auth
    location /api/ {
        auth_request /auth;
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # Auth endpoint
    location = /auth {
        internal;
        proxy_pass http://localhost:3001/auth;
        proxy_pass_request_body off;
        proxy_set_header Content-Length "";
        proxy_set_header X-Original-URI $request_uri;
        proxy_set_header Authorization $http_authorization;
    }
}
```

## 2. Environment Variables

```bash
# .env.ai-server
AI_SERVER_TOKEN=your-super-secret-token-here
OLLAMA_MODELS_PATH=/data/ollama/models
NGINX_AUTH_ENABLED=true
RATE_LIMIT_PER_MINUTE=60
```

## 3. Docker Compose

```yaml
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - ollama
    restart: unless-stopped

  auth-service:
    build: ./auth-service
    environment:
      - AI_SERVER_TOKEN=${AI_SERVER_TOKEN}
    ports:
      - "3001:3001"
    restart: unless-stopped
```

## 4. Deployment Options Comparison

### üöÄ RunPod (Aj√°nlott)
- **K√∂lts√©g:** $0.34/√≥ra (RTX 4090)
- **Setup:** ~10 perc Docker deploy
- **Sk√°l√°z√°s:** Auto-scale
- **El≈ëny:** Nincs hardware befektet√©s

### üíª Saj√°t Szerver
- **K√∂lts√©g:** $1500-3000 egyszer
- **Setup:** ~2 √≥ra konfigur√°ci√≥  
- **Sk√°l√°z√°s:** Manu√°lis
- **El≈ëny:** Teljes kontroll

### ‚òÅÔ∏è Lambda Labs
- **K√∂lts√©g:** $1.10/√≥ra (A100)
- **Setup:** ~5 perc
- **Sk√°l√°z√°s:** Jupyter-style
- **El≈ëny:** Pr√©mium GPU-k

## 5. Security Checklist

- ‚úÖ Bearer token auth
- ‚úÖ Rate limiting (10/min/IP)
- ‚úÖ HTTPS only
- ‚úÖ Nginx proxy
- ‚úÖ No model switching endpoint
- ‚úÖ Request size limits
- ‚úÖ Timeout protection

## 6. Monitoring

```bash
# Health check endpoint
curl -H "Authorization: Bearer $TOKEN" \
  https://your-ai-server.com/api/tags

# Performance monitoring  
docker stats ollama nginx
```